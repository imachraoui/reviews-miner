{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_to_keep =[r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",# hash-tags\n",
    "                r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\" #smileys\n",
    "                ]\n",
    "\n",
    "regex_to_keep=  r'('+'|'.join(groups_to_keep)+')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_motifs = pandas.DataFrame(np.transpose([motifs,motifs]),columns=['micro_src','micro_dest'])\n",
    "seuil_confiance = 90\n",
    "\n",
    "def fuzzy_logic(motif):\n",
    "    resultat = process.extractOne(motif, themes_edf[\"Désignation\"])\n",
    "    if resultat[1] > seuil_confiance :\n",
    "        return resultat[0]\n",
    "    else : \n",
    "        return np.nan\n",
    "\n",
    "def get_macro_category(micro):\n",
    "    if not pandas.isnull(micro) :\n",
    "        return themes_edf[\"Thème/Motif\"][themes_edf[\"Désignation\"]== micro].values[0]\n",
    "    else:\n",
    "        return 'Autres'\n",
    "    \n",
    "mapping_motifs['micro_dest'] = mapping_motifs['micro_dest'].map(fuzzy_logic)\n",
    "mapping_motifs['macro'] =  mapping_motifs['micro_dest'].copy(deep=True)\n",
    "\n",
    "mapping_motifs['macro'] = mapping_motifs['macro'].map(get_macro_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_patterns(verbatim) :\n",
    "    if re.search(regex_to_delete, verbatim) :\n",
    "        return re.sub(regex_to_delete, \"\", verbatim) \n",
    "    return verbatim\n",
    "\n",
    "def no_accents(s):\n",
    "    s= unicodedata.normalize('NFD',s)\n",
    "    return str(s.encode('ascii','ignore'), 'utf-8')\n",
    "\n",
    "groups_to_delete = [r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "                    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)',# numbers\n",
    "                    r'([_\\-]+)' # underscores and dashes\n",
    "    ] \n",
    "\n",
    "                 \n",
    "regex_to_delete = r'('+'|'.join(groups_to_delete)+')'\n",
    "\n",
    "word_tokenizer = RegexpTokenizer('\\w+', gaps=False)\n",
    "\n",
    "frenchStemmer= SnowballStemmer('french', ignore_stopwords=True)\n",
    "\n",
    "stop_words_french = stopwords.words('french')\n",
    "stop_words_french_normalized =[no_accents(stop) for stop in stop_words_french]\n",
    "\n",
    "JAR_PATH = 'C:/Users/MACH055/Documents/code/stanford-postagger-full-2015-12-09/stanford-postagger.jar'\n",
    "French_MODEL_PATH = 'C:/Users/MACH055/Documents/code/stanford-postagger-full-2015-12-09/models/french.tagger'\n",
    "FrenchPostagger = StanfordPOSTagger(French_MODEL_PATH, JAR_PATH, encoding='utf8')\n",
    "\n",
    "french_tagger = treetaggerwrapper.TreeTagger(TAGLANG='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, cleaning=True, stemming=True, pos_tagging_lemmatization=False):\n",
    "    clean_verbatims = list()\n",
    "    pos_tagged = list()\n",
    "    \n",
    "    # POS-Tagging\n",
    "    #if pos_tagging :\n",
    "     #   sentences = [str(v).split() for v in data]\n",
    "    #  pos_tagged = FrenchPostagger.tag_sents(sentences)\n",
    "       \n",
    "    \n",
    "    for verbatim in data :\n",
    "        \n",
    "        # remove numbers, html ...\n",
    "        if cleaning :\n",
    "            verbatim_i = remove_patterns(str(verbatim))\n",
    "        \n",
    "        # POS-Tagging\n",
    "        if pos_tagging_lemmatization :\n",
    "            temp_pos = french_tagger.tag_text(verbatim_i)\n",
    "            tags = treetaggerwrapper.make_tags(temp_pos)\n",
    "            pos_tagged.append(tags)\n",
    "            \n",
    "            # Fonction permettant d'obtenir le lemme d'un objet Tag ou recuperer le mot si un objet NotTag\n",
    "            getlemma_pos = lambda x : operator.attrgetter('lemma')(x) if (\n",
    "                isinstance(x,treetaggerwrapper.Tag)) else operator.attrgetter('what')(x)\n",
    "            verbatim_i = ' '.join([getlemma_pos(tag) for tag in tags])\n",
    "            \n",
    "        # remove accents \n",
    "        tokens = word_tokenizer.tokenize(no_accents(verbatim_i))\n",
    "            \n",
    "        # remove stop words + to lower case + stemming\n",
    "        if stemming :\n",
    "            tokens_w_s = [frenchStemmer.stem(i.lower()) for i in tokens if (i not in stop_words_french_normalized) & (len(i)>2)]\n",
    "            \n",
    "        if pos_tagging_lemmatization :\n",
    "            tokens_w_s = [i for i in tokens if (i not in stop_words_french_normalized) & (len(i)>2)]\n",
    "            \n",
    "        # concat words\n",
    "        clean_verbatims.append(' '.join(tokens_w_s))\n",
    "        \n",
    "    return clean_verbatims, pos_tagged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
